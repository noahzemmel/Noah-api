# balanced_content_generator.py - Balanced speed and quality content generation
import asyncio
import aiohttp
import time
import json
from typing import List, Dict, Any, Optional
import os
from datetime import datetime, timedelta
import hashlib

class BalancedContentGenerator:
    def __init__(self):
        self.content_cache = {}
        self.cache_file = "balanced_cache.json"
        self.load_cache()
        
    def load_cache(self):
        """Load content cache for speed optimization"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r') as f:
                    self.content_cache = json.load(f)
        except:
            self.content_cache = {}
    
    def save_cache(self):
        """Save content cache"""
        try:
            with open(self.cache_file, 'w') as f:
                json.dump(self.content_cache, f, indent=2)
        except:
            pass
    
    def get_cache_key(self, topics: List[str], language: str, duration: int) -> str:
        """Generate cache key for content"""
        key_data = {
            "topics": sorted(topics),
            "language": language,
            "duration": duration,
            "date": datetime.now().strftime("%Y-%m-%d")
        }
        return hashlib.md5(json.dumps(key_data, sort_keys=True).encode()).hexdigest()
    
    def get_cached_content(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """Get cached content if available and recent"""
        if cache_key in self.content_cache:
            cached = self.content_cache[cache_key]
            # Check if cache is less than 1 hour old (shorter for news freshness)
            cache_time = datetime.fromisoformat(cached["cached_at"])
            if datetime.now() - cache_time < timedelta(hours=1):
                return cached["content"]
        return None
    
    def cache_content(self, cache_key: str, content: Dict[str, Any]):
        """Cache generated content"""
        self.content_cache[cache_key] = {
            "content": content,
            "cached_at": datetime.now().isoformat()
        }
        # Keep only last 50 cached items for memory efficiency
        if len(self.content_cache) > 50:
            oldest_key = min(self.content_cache.keys(), 
                           key=lambda k: self.content_cache[k]["cached_at"])
            del self.content_cache[oldest_key]
        self.save_cache()
    
    async def fetch_news_balanced(self, topics: List[str], session: aiohttp.ClientSession) -> List[Dict[str, Any]]:
        """Fetch news with balanced speed and quality"""
        start_time = time.time()
        
        try:
            # Use parallel processing for multiple topics
            tasks = []
            for topic in topics:
                task = self.fetch_topic_news_balanced(topic, session)
                tasks.append(task)
            
            # Wait for all topics to complete (but with timeout)
            results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=15.0  # 15 second timeout for news fetching
            )
            
            # Process results
            all_articles = []
            for result in results:
                if isinstance(result, list):
                    all_articles.extend(result)
                elif isinstance(result, Exception):
                    print(f"Error fetching news: {result}")
            
            # Remove duplicates and sort by relevance
            unique_articles = self.remove_duplicates(all_articles)
            sorted_articles = sorted(unique_articles, 
                                  key=lambda x: x.get('relevance_score', 0), 
                                  reverse=True)
            
            fetch_time = time.time() - start_time
            print(f"ðŸ“° News fetched in {fetch_time:.2f}s: {len(sorted_articles)} articles")
            
            return sorted_articles[:8]  # Limit to top 8 articles for quality
            
        except asyncio.TimeoutError:
            print("â° News fetching timed out, using fallback")
            return self.get_fallback_news(topics)
        except Exception as e:
            print(f"âŒ Error in news fetching: {e}")
            return self.get_fallback_news(topics)
    
    async def fetch_topic_news_balanced(self, topic: str, session: aiohttp.ClientSession) -> List[Dict[str, Any]]:
        """Fetch news for a single topic with balanced approach"""
        try:
            # Use optimized search queries for better coverage
            search_queries = [
                f"breaking news {topic} last 24 hours",
                f"latest {topic} developments today",
                f"{topic} recent updates"
            ]
            
            all_articles = []
            for query in search_queries:
                try:
                    # This would integrate with your actual Tavily API
                    articles = await self.call_tavily_api(query, session)
                    all_articles.extend(articles)
                except Exception as e:
                    print(f"Error with query '{query}': {e}")
                    continue
            
            return all_articles
            
        except Exception as e:
            print(f"Error fetching news for topic '{topic}': {e}")
            return []
    
    async def call_tavily_api(self, query: str, session: aiohttp.ClientSession) -> List[Dict[str, Any]]:
        """Call Tavily API with optimized parameters"""
        try:
            # This is where you'd integrate with your actual Tavily API
            # For now, simulate the API call with realistic timing
            
            # Simulate network delay (realistic for API calls)
            await asyncio.sleep(0.5)
            
            # Mock response structure (replace with actual API call)
            mock_articles = [
                {
                    "title": f"Latest {query} news",
                    "url": f"https://example.com/{query.replace(' ', '-')}",
                    "content": f"Recent developments in {query} with specific details and updates",
                    "relevance_score": 0.9,
                    "topic": query.split()[0],
                    "published_date": datetime.now().isoformat()
                }
            ]
            
            return mock_articles
            
        except Exception as e:
            print(f"Error calling Tavily API: {e}")
            return []
    
    def remove_duplicates(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove duplicate articles based on URL and content similarity"""
        seen_urls = set()
        seen_titles = set()
        unique_articles = []
        
        for article in articles:
            url = article.get('url', '')
            title = article.get('title', '').lower()
            
            # Check for duplicates
            if url and url not in seen_urls and title not in seen_titles:
                seen_urls.add(url)
                seen_titles.add(title)
                unique_articles.append(article)
        
        return unique_articles
    
    def get_fallback_news(self, topics: List[str]) -> List[Dict[str, Any]]:
        """Get fallback news when API calls fail"""
        fallback_articles = []
        for topic in topics:
            fallback_articles.append({
                "title": f"Recent {topic} developments",
                "url": f"https://news.google.com/search?q={topic}",
                "content": f"Latest updates in {topic} from reliable sources",
                "relevance_score": 0.7,
                "topic": topic,
                "published_date": datetime.now().isoformat()
            })
        return fallback_articles
    
    def generate_balanced_prompt(self, topics: List[str], language: str, duration: int, tone: str, articles: List[Dict[str, Any]]) -> str:
        """Generate balanced prompt for quality content generation"""
        target_words = duration * 150  # Approximate words per minute
        
        # Include article context in prompt for better quality
        article_context = ""
        if articles:
            article_context = f"\nAVAILABLE ARTICLES:\n"
            for i, article in enumerate(articles[:5], 1):
                article_context += f"{i}. {article.get('title', 'No title')}\n"
                article_context += f"   Content: {article.get('content', 'No content')[:200]}...\n\n"
        
        prompt = f"""Generate a {duration}-minute news briefing in {language} with a {tone} tone.

TOPICS: {', '.join(topics)}

{article_context}

REQUIREMENTS:
- EXACTLY {target_words} words (Â±15 words)
- Focus on SPECIFIC, RECENT updates from last 24 hours
- Use information from the provided articles
- Avoid generic overviews - focus on what HAPPENED
- Include specific facts, numbers, and developments
- Use clear, engaging language
- Structure: Brief intro, main points with details, conclusion

FORMAT: Direct, informative, engaging
STYLE: Professional but accessible
LENGTH: {target_words} words exactly

Generate the briefing now:"""

        return prompt
    
    async def generate_content_balanced(self, topics: List[str], language: str, duration: int, tone: str) -> Dict[str, Any]:
        """Generate content with balanced speed and quality"""
        start_time = time.time()
        
        # Check cache first
        cache_key = self.get_cache_key(topics, language, duration)
        cached_content = self.get_cached_content(cache_key)
        if cached_content:
            print(f"âš¡ Cache hit! Content generated in {time.time() - start_time:.2f}s")
            return cached_content
        
        try:
            # Step 1: Fetch news (parallel, with timeout)
            async with aiohttp.ClientSession() as session:
                news_results = await self.fetch_news_balanced(topics, session)
            
            # Step 2: Generate optimized prompt
            prompt = self.generate_balanced_prompt(topics, language, duration, tone, news_results)
            
            # Step 3: Generate content (this would integrate with your OpenAI API)
            content = await self.generate_final_content_balanced(prompt, news_results, duration, language)
            
            # Cache the result
            self.cache_content(cache_key, content)
            
            generation_time = time.time() - start_time
            print(f"ðŸš€ Balanced content generated in {generation_time:.2f}s")
            
            return content
            
        except Exception as e:
            print(f"âŒ Error in balanced generation: {e}")
            # Return fallback content
            return self.get_fallback_content(topics, language, duration)
    
    async def generate_final_content_balanced(self, prompt: str, news: List[Dict[str, Any]], duration: int, language: str) -> Dict[str, Any]:
        """Generate final content with balanced approach"""
        # This would integrate with your actual OpenAI API
        # For now, create realistic content structure
        
        word_count = duration * 150
        
        # Create realistic transcript based on news
        transcript_parts = []
        transcript_parts.append(f"Welcome to the {duration}-minute news briefing. Here are the latest updates:")
        
        for i, article in enumerate(news[:5], 1):
            topic = article.get('topic', 'general')
            title = article.get('title', 'Breaking news')
            transcript_parts.append(f"\n{i}. {topic.upper()}: {title}")
            transcript_parts.append(f"   {article.get('content', 'Details coming soon')}")
        
        transcript_parts.append(f"\nThat concludes our {duration}-minute briefing. Stay informed and have a great day.")
        
        transcript = "\n".join(transcript_parts)
        
        content = {
            "transcript": transcript,
            "word_count": word_count,
            "target_duration_minutes": duration,
            "estimated_duration_minutes": duration,
            "precision_timing": True,
            "news_quality": {
                "quality_score": 85.0,
                "recent_articles": len(news),
                "total_articles": len(news),
                "high_relevance_articles": len([a for a in news if a.get('relevance_score', 0) > 0.8]),
                "topics_with_news": [n.get('topic', '') for n in news if n.get('topic')],
                "topics_without_news": [],
                "has_recent_news": True
            },
            "sources": news,
            "generation_metadata": {
                "cache_used": False,
                "parallel_processing": True,
                "optimization_level": "balanced",
                "news_fetch_time": "optimized",
                "content_quality": "high"
            }
        }
        
        return content
    
    def get_fallback_content(self, topics: List[str], language: str, duration: int) -> Dict[str, Any]:
        """Get fallback content when generation fails"""
        word_count = duration * 150
        
        fallback_transcript = f"""Welcome to the {duration}-minute news briefing in {language}.

We're experiencing technical difficulties with our news sources, but here's what we know about your requested topics: {', '.join(topics)}.

Please try again in a few minutes for the latest updates and breaking news.

Thank you for your patience."""
        
        return {
            "transcript": fallback_transcript,
            "word_count": len(fallback_transcript.split()),
            "target_duration_minutes": duration,
            "estimated_duration_minutes": duration,
            "precision_timing": False,
            "news_quality": {
                "quality_score": 30.0,
                "recent_articles": 0,
                "total_articles": 0,
                "high_relevance_articles": 0,
                "topics_with_news": [],
                "topics_without_news": topics,
                "has_recent_news": False
            },
            "sources": [],
            "generation_metadata": {
                "cache_used": False,
                "parallel_processing": False,
                "optimization_level": "fallback",
                "error_recovery": True
            }
        }
    
    def cleanup(self):
        """Cleanup resources"""
        self.save_cache()

# Global balanced instance
balanced_generator = BalancedContentGenerator()
